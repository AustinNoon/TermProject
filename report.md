# Report
## By: Austin Noon, Joshua Lawson, Matthew Carroll, Ruben Germosen
### Date: November 29, 2023 

Introduction To Sorting Algorithms
	Before jumping into the exploration of sorting algorithms and their significance, it is important to define and understand what an algorithm is. For this there are numerous definitions, however we will look into what it means within computer science. An algorithm can be defined as “a procedure for solving a computational problem” (Eldridge). It is important to note that if one is trying to solve a problem with an algorithm, the algorithm must be designed to handle that problem specifically. 
Now having known what an algorithm is, we can define a sorting algorithm. In the practices of computer science, a sorting algorithm can be simply defined as “a procedure for ordering elements in a list by repeating a sequence of steps” (Eldridge). There are many different types of sorting algorithms that can be implemented, meaning that one could sort a given data set in many different ways. Since we have now established a very fundamental, yet minimal, understanding of sorting algorithms within the realm of computer science, we can start to explore the world of sorting algorithms more in depth. Before looking into the significance of sorting algorithms, it is essential to go into greater detail about their background. 
1.1 Basic Background of Sorting Algorithms
	As we have established, a sorting algorithm is used to sort a list of elements in a particular fashion. Typically, they are used to make said list be more accessible and convenient than it was prior to being sorted. However the need and use of sorting algorithms expands far beyond just merely making datasets look neater and be more accessible. The need for sorting algorithms becomes abundantly clear when a diverse application that requires data to be sorted in a specific order to facilitate efficient searching, data retrieval or data analysis. A great example of one of these diverse applications would be the world’s most popular search engine, Google. Google has roughly 8.5 billion searches per day, and by noting the amount of seconds per day, you can perform the calculation and come to the conclusion that Google services roughly 98,000 searches per second, and in most cases returns search results in under two tenths of a second (Mohsin). This is done by implementing sorting algorithms with the lowest possible time complexity in order to satisfy their users. 
	There are multiple basic fundamentals of sorting algorithms we must explore in order to gather a strong background relative to the topic. In many real world scenarios, data is more likely to arrive in an unsorted fashion rather than a sorted fashion. This means that upon arrival of the data, the data set is going to be unsorted and unstructured. For example, consider a class list of students, or a database of customer information. This information, when unsorted or without a defined order of any type, can pose challenges for efficient processing. Sorting algorithms can address these challenges by providing the data with a structure, which in turn will make the data easier to work with as well as improving the efficiency of processing. Another useful characteristic of sorting algorithms is the flexibility of sorting criteria. These algorithms can be based on various criteria, usually depending on the nature of the data and the requirements of the application. Some examples of standard sorting criteria are as follows: ascending or descending order, numerical order, and alphabetical order. This adaptability allows sorting algorithms to be versatile and applicable within a wide range of scenarios. 
	Aside from sorting criteria, there are other key characteristics of sorting algorithms that are to be considered. Different algorithms exhibit different characteristics, and understanding these characteristics is pivotal for selecting the most suitable algorithm for a given task. Several factors contribute to the uniqueness of sorting algorithms, in turn influencing their performance and applicability across different scenarios. Time complexity is the first of these characteristics we will look at. Time complexity can be defined as “a description of how much computer time is required to run an algorithm” (Eldridge). In other words, it is a fundamental metric that measures the efficiency of sorting algorithms in terms of the time it takes to complete the sorting process. Time complexity employs the use of certain mathematical notations to represent the run time of an algorithm, and these notations are based on the size of the input into the algorithm. These notations are represented as T(n), where n signifies the input size, and these are further represented in Big-O notation, where the growth of the function is referenced (Eldridge). Certain algorithms are favorable in comparison to others in terms of time complexity, but it depends on the type of data and or pattern. 
	Space complexity is time complexity’s counterpart, and the two of them make up what is known as computational complexity. Space complexity is another key characteristic of sorting algorithms, as it evaluates the amount of memory or storage that is required by the algorithm. For this there are two classifications, “in-place” and “out-of-place”, where each of these have implications for memory usage and resource efficiency. An in-place sorting algorithm will operate directly on the input data without requiring additional memory proportional to the size of the data set, which in turn makes these types of algorithms favorable in terms of memory-efficiency (“What Are In-Place and Out-of-Place Algorithms?”). Take Selection Sort for example, it works by iteratively selecting the minimum element from the unsorted portion of the array and swapping it with the first unsorted element, and continues this process until the entire array is sorted. On the other hand, out-of-place sorting algorithms require additional memory to store the sorted elements, usually by way of creating a separate copy of the input data or employing auxiliary data structures. The trade off to these algorithms using more memory than in-place algorithms is that they have the advantage in stability, which is the next characteristic to be covered. These out-of-place algorithms can be very advantageous in scenarios where modifying the original data might not be permitted. An example of an out-of-place algorithm would be Merge Sort, where an input array is divided into smaller segments until single element arrays are obtained. The sorted pairs of elements are then merged to produce larger sorted arrays until the entire data set is sorted. We will explore Merge Sort in a more detailed fashion later in this paper.
	As mentioned in the previous paragraph, the stability of an algorithm is yet another very important characteristic of a sorting algorithm. The stability of a sorting algorithm is “the label that indicates whether or not it will preserve the order of two or more elements in the array if they have equal sorting keys” (Stringer). This characteristic proves its importance in scenarios where the initial order of equivalent elements holds significance. For example, if we have a data set containing student’s names and grades, a stable sort would ensure that students with the same grade would remain in the original name order. 
	The final two characteristics to be addressed are adaptability and suitability for data size and distribution. Adaptability is simply how well the algorithm works across various input conditions. Some sorting algorithms are adaptable to the characteristics of the input data they are processing, which makes them versatile, and sometimes favorable, depending on the situation. Again as an example we can visit Merge Sort, which is known to exhibit a consistent level of performance and adaptability across a wide range of data distributions, which makes it a reliable choice for diverse data sets. The size and distribution of data can significantly impact the performance of sorting algorithms. For example, insertion sort demonstrates efficiency when handling smaller data sets, but that efficiency gradually decreases as the size of the data set increases. In the case where a much larger data set is seeking to be sorted, it would be optimal to use a technique such as parallel sorting where multiple processing cores are being used to expedite the sorting process. 
	To conclude our insight into the background of sorting algorithms we should look at the overall impact sorting algorithms have on a systems performance. We have mentioned efficiency many times prior to this paragraph, but we have yet to dive into the details of efficiency and how different algorithms can have an impact on a system’s performance. This impact is many-sided, as it will influence response times, resource utilization, and the user’s experience. Efficient sorting algorithms provide a heavy contribution to the responsiveness of software applications. For example, we can look back to the Google Search example from earlier. Due to Google being the most widely used search engine in the world, they have to employ algorithms with a very low time complexity, as this will increase the system’s response time. In general, search engines are subjected to navigating through a very dense amount of information, and sorting algorithms with an optimal response time  make up the building blocks of these search engines. Sorting algorithms also have an impact on resource utilization, specifically power consumption and memory usage. As we previously mentioned, algorithms with a lower space complexity might be found preferable in an environment with limited memory resources, while algorithms with an efficient time complexity can contribute to an optimal utilization of a processor. A good example of this to look at would be server-side sorting. “When server-side filtering or sorting is performed, the application sends a query that is a modification of the starting query to the server. The shortcoming of this way is that server resources are taken, but the benefit is that a user will get a complete result of sorting or filtering” (“Server-Side and Client-Side Sorting and Filtering.”). From this we can infer that server-side sorting algorithms impact the speed at which the queries are executed, which in turn can reduce processing times and allow the database to handle a larger amount of concurrent queries at the same time. 
	In summary, sorting algorithms form an extremely important foundation of computer science, as they address the always important issue of dealing with unsorted data by structuring said data and making datasets more manageable. Their flexibility in accommodating various sorting criteria, whether based on numerical, alphabetical, or a custom criteria, underscores their adaptability to different applications’ needs. Understanding key algorithmic characteristics such as time complexity, space complexity, stability, and adaptability empowers developers to make informed decisions that contribute to efficient and optimized computational processes. Their impact on system performance extends to responsive software, fast search results, and improved user experiences. In a wide range of applications, sorting algorithms play a pivotal role in enhancing the functionality of programs, making them an indispensable tool in the world of computer science. 
1.2 Why Are Sorting Algorithms Useful?
	Now that we have a knowledgeable and meaningful understanding of the background of sorting algorithms, we can further explore how and why they are so useful. We touched upon this subject in the last section, but we shall explore it in further detail. There are three main topics to be discussed within this section: enhancing data organization, improved search and retrieval, and preparing data for further processing.  These three topics provide a good basis to understanding why sorting algorithms are so important in the world of computer science and real-world applications. 
	Sorting algorithms play a pivotal role in enhancing the organization of data, addressing the common challenge of dealing with unordered datasets prevalent in real world applications. As we have addressed, datasets are in an unsorted manner most of the time, and this poses challenges for efficient data management. Sorting algorithms pose as a systematic and quick response to address the issues, as they allow for rearrangement of the data based on the specific criteria of however the dataset should be sorted. For this, we can look at a scenario where a business receives a large customer dataset. When this data arrives as an unsorted dataset, it is challenging to extract any meaningful information from it. However by implementing sorting algorithms, it would theoretically become much easier to navigate the dataset by making it structured and organized. 
	The organized structure that results from sorting algorithms offers several different advantages such as simplifying data maintenance and identifying trends and patterns within the data. If we revisit the previous example of a large customer dataset, and sort by date, it would provide a quick analysis of sales trends over time. Simplifying the data maintenance is of equal importance. Simplifying data maintenance makes information easily accessible, and this is crucial if the dataset in question is updated regularly. Furthermore, having good data maintenance and organized data makes future tasks easier, such as searching for a specific record, or merging a dataset with another.
	Sorting algorithms have a profound impact on search and retrieval processes, which in turn makes them play a pivotal role in information retrieval systems. Firstly, an information retrieval system can be defined as, “a software program that deals with the organization, storage, retrieval, and evaluation of information from document repositories, particularly textual information” (Siddhi). A simpler, non-software example of an information retrieval system we could look at is a dictionary. In a dictionary, words are arranged in alphabetical order, and this method of organization allows the user to navigate through the dictionary swiftly and find whatever they need to. In the digital world, search engines use the power of sorting algorithms to bring a similar level of efficiency to information retrieval. This again loops back to the example of Google Search from earlier and how they implement sorting algorithms, but now we focus on how Google presents the search results based off of the user’s query. When a user initiates a search, the search engine rapidly sifts through massive datasets, applying sorting algorithms to rank the results based on relevance (“How Results Are Automatically Generated”). This approach demonstrates how sorting algorithms are vital to the search and retrieval process.
	Sorting algorithms serve as a preliminary step in preparing data for more complex operations and analyses. Before being able to apply sophisticated algorithms or conduct in-depth data analyses, it is often important to preprocess data through sorting algorithms. Having the data be sorted prior to applying some complex algorithm to it makes sure that the data will be in a suitable format for tasks such as searching or merging. 
Introduction to Our Project
	For our term project, our group decided to implement the three required sorting algorithms: insertion sort, and mergesort, quicksort, and chose bucket sort as our additional sorting algorithm. Our group wrote a datasort.h and a datasort.cpp file that contains both the four sorting algorithms listed above and additional helper functions needed to do additional tasks such as taking in the data from a file and to find the runtime. We also were required to create a Github repository to store the code, this was very helpful in practicing with Github as well as for collaboration between group members. It is very beneficial that Github allows you to see prior commits. They show you what has been changed within the file along with each commit’s title, message, the user that committed and the time of the commit. The purpose for this project is to deepen our understanding around sorting algorithms, not just the structure of the code and how to write them but why they are useful on a larger scale. It is important to understand the strengths and limitations of similar sorting algorithms and by working on this project, it forces us to understand the content on a deeper level.
2.1 Why Did We Choose This Topic?
	The biggest factor that drew us to this topic was the practicality of sorting algorithms. They are a fundamental building block in programming and are necessary in any circumstance where data sets are involved. In section 1 we went into depth on why sorting algorithms are useful, they are utilized in a multitude of professions and directly correlate to the success of dataset analytics. Here are a few more real world applications, one being online retail. These algorithms organize product listings and can be used to enhance the user's experience by enabling specific comparisons based on criteria like price and popularity. Another example is in healthcare records. They benefit from sorting algorithms to ensure that patient data is easily accessible for medical professionals. In the realm of finance and banking, sorting algorithms are used for tasks such as sorting transactions by date or amount. The likelihood that all of us encounter sorting algorithms in our future professions, whatever they may be, is almost guaranteed, which was the driving factor behind us choosing this topic.
2.2 How Do Data Sets And Sorting Algorithms Coincide?
Data sets and sorting algorithms coincide in the realm of data organization and manipulation. Data sets often require a specific arrangement of the data for efficient processing and further use. Sorting algorithms provide the mechanisms to arrange this data, ensuring that it is organized in a specified order, such as numerical or alphabetical. The choice of sorting algorithm depends on the characteristics of the data set and the efficiency requirements of the application. Some algorithms, such as insertion sort, are simple and concise which is perfect for sorting small sets of data. In section 1.1, we went into depth on time complexity, and because insertion sort has a time complexity O(n^2), it is not efficient for large datasets making algorithms such as bucket sort a much better suited option if memory is not a limiting factor (Eldridge). Whether it's organizing financial transactions, patient records in healthcare, or product listings in e-commerce, the alignment of data sets with appropriate sorting algorithms is essential for optimizing search, analysis, and retrieval operations, contributing significantly to the success of various computational tasks that require specifically organized data.
Methods Implemented In Our Project
As mentioned in the previous section, our group implemented four different sorting algorithms: quicksort, insertion sort, mergesort, and bucket sort. These algorithms serve as the backbone to our data processing program, influencing its efficiency and adaptability in handling datasets of varying sizes and complexities. Before exploring how we implemented these algorithms and their corresponding helper functions, it is very important to gain a strong understanding of these sorting algorithms. Each of these algorithms brings its own set of characteristics, advantages, and optimal use cases, contributing to the versatility of our approach. The understanding of these algorithms lays the groundwork for a detailed exploration of the implementation strategies we used in our project. 
3.1 Insertion Sort
	Insertion sort is a fundamental and intuitive sorting algorithm, and it operates on the principle of building a sorted sequence gradually. To look at it from its algorithmic standpoint, it focuses on one element at a time. During each iteration, the algorithm compares the current element with those already sorted, seamlessly finding its appropriate position within the sequence. This process then repeats until each element in the array has found its respective, sorted position. Like any algorithm, insertion sort has its own key characteristics. Insertion sort stands out because of its simplicity, which renders the algorithm as a useful choice for when an unsophisticated yet efficient sorting algorithm is sought. The uncomplicated nature of the algorithm tends to ease in development, as well as making it a great introductory choice when studying sorting algorithms.  The algorithm’s adaptability to partially sorted data is a distinct strength that stands out amongst other algorithms, which makes it sufficient and sometimes favorable in scenarios where the data may already be partially sorted to some degree (“Insertion Sort - Data Structure and Algorithm Tutorials”). Insertion sort is also a very memory efficient algorithm, with its classification being specifically an in-place algorithm, as mentioned earlier. To refresh and clarify, this means that it rearranges the elements within the existing array without requiring additional memory space proportional to the size of the input data. This characteristic would make insertion sort favorable in a situation where memory access is limited, and makes this algorithm suitable for sorting small to moderately-sized data sets.
	Stability was also mentioned earlier, but again to refresh the stability of an algorithm in simplistic terms is the preservation of relative order of equal elements. By this definition, insertion sort is a stable algorithm, so if two elements in the array have an equal value, their original order will be maintained in the sorted output (“Insertion Sort - Data Structure and Algorithm Tutorials”). This characteristic can prove important in certain situations, for example when the original order of elements carries significance. Lastly, the time complexity of any algorithm bears three cases: “Best Case”, “Average Case”, and “Worst Case”. In the worst case scenario, specifically when the list and or array has to be fully traversed, or if the input array is in reversed order, insertion sort exhibits a quadratic time complexity, O(n2). For each element in the array, it needs to compare and swap with every preceding element, which results in a nested loop structure, and so as a result, the worst case time complexity is proportional to the square of the input size (“Insertion Sort Algorithm”). From this we can see why insertion sort is not a good choice if the input size is large. The best case scenario for insertion sort occurs when the input array is already sorted, or if there is no sorting that needs to be done (“Insertion Sort Algorithm”). In this situation, we can infer that the algorithm will make a single iteration through the array, comparing each element with its predecessor and making no swaps. The best case time complexity for insertion sort is linear, being represented by O(n) (“Insertion Sort Algorithm”).
Lastly, the average case time complexity of insertion sort is represented by a quadratic again, O(n2). This will occur when the elements in the array are in a mixed order, i.e. not ascending but also not descending. 
3.2 Mergesort 
Mergesort is a more complex algorithm than insertion sort, and it operates quite differently. When looking at mergesort from an algorithmic standpoint, we can see that it uses a divide and conquer strategy. It recursively divides the input array into two halves until each sub-array only consists of one element. These single element arrays are then merged back together in a sorted order (“Merge Sort - Data Structure and Algorithms Tutorials”). The recursive division and merging process is continuously repeated until the entire array is sorted. Mergesort is an algorithm that is known for its consistent performance across various input scenarios. The divide and conquer nature of the algorithm allows it to handle large, medium, and small sized data sets in the same fashion. Similar to insertion sort, mergesort is a stable algorithm, and as covered in the previous section, this can be important in scenarios where the relative order of equal elements is preserved during the sorting process. 
The time complexity of mergesort is rather predictable, specifically because the best, worst, and average case scenarios are all the same expression, O(n log n) (“Merge Sort - Data Structure and Algorithms Tutorials”). To break down the derivation of this expression, we can view it like this: a merge sort consists of numerous iterations over the input, where in the first iteration, the algorithm merges segments of size one, and in the second iteration it merges segments of size two, and this goes on until the nth iteration where it merges segments of size 2n-1, which results in the total number of iterations being log(n). Combine this with the fact that we can merge two sorted arrays in linear time, which we know is O(n) from the previous section, and we end up with O(n log n) as the time complexity for any case of mergesort. 
Mergesort is also particularly useful when sorting large data sets, as well as linked lists, and this is mainly due to its divide and conquer strategy. This strategy allows the algorithm to break one large task into multiple smaller sub-tasks, and this can reduce the time taken to run the algorithm. Mergesort also has the advantage of being adaptable to external sorting, which can be defined as “a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted does not fit into the main memory of a computing device (usually RAM) and instead, must reside in the slower external memory” (“External Sorting”). Mergesort can efficiently perform its sorting process using external storage, such as disk drives, and its adaptability to do so can be valuable if you have to deal with datasets that exceed the available amount of RAM. However, mergesort is considered an out-of-place sorting algorithm because it requires additional memory, which can be a problem in an application where memory usage is limited (“Merge Sort - Data Structure and Algorithms Tutorials”). Lastly, another unique feature of mergesort is that it has the potential to be parallelized. This feature of sorting algorithms was touched on briefly earlier, but to refresh it means that the processing of sub-problems can happen concurrently through multiple processors, which can potentially lead to improved sorting performance. 
3.3 Quicksort
	Similar to mergesort, quicksort is also a divide and conquer type sorting algorithm. When looking at the logic of quicksort, we see that it begins by selecting a pivot element from the array. It is important to note that the choice of the pivot can significantly impact the algorithm’s efficiency. Some of the most common choices of pivot include the first element, the last element, or the middle element. After the pivot element is chosen, the array is partitioned into two sub-arrays based on the value of the pivot. The elements that are less than the pivot are placed on the left, while the elements that are greater than the pivot are placed on the right. This step of partitioning the array into subarrays is recursive and very important to the efficiency of quicksort. The end goal of this algorithm is to place the pivot in its final sorted position while ensuring that elements less than the pivot are placed on the left, and elements greater than the pivot are placed to the right of the pivot. As mentioned, the partitioning and pivot selection is recursive and will only stop when the base case is reached, typically when a sub-array has one or zero elements left. When the base case is reached, the sorted subarrays are combined to produce a final sorted result (“Quicksort - Data Structure and Algorithm Tutorials”).
	The time complexity of quicksort remains the same for best and average case scenarios, but severely drops off for worst case scenarios. The best case scenario of quicksort happens when the pivot that is chosen is able to divide the array into almost equal subarrays. This leads to balanced partitions, which in turn leads to efficient sorting, and the complexity is represented as O(n log n) (“Quicksort - Data Structure and Algorithm Tutorials”). As previously mentioned, the average case scenario for quicksort is the same as the best case scenario, so it is also represented as O(n log n). The worst case scenario for quicksort occurs when the pivot repeatedly results in extremely unbalanced partitions, such as if the input is already sorted, the elements of the array or equal, or if the input is in reversed order. The time complexity of the worst case scenario is denoted as O(n2) (“Quicksort - Data Structure and Algorithm Tutorials”).
	If the worst case can be avoided, either by using a different algorithm or a method such as three way partitioning, quicksort is a very efficient sorting algorithm for a very broad range of input scenarios, excluding scenarios where the input is small,  and it is regarded as one of the fastest sorting algorithms at our disposal (“Quicksort - Data Structure and Algorithm Tutorials”). Quicksort also often operates as an in-place algorithm, as it rearranges elements within an existing array without requiring any additional memory space proportional to the input size. Although it is an in-place algorithm, quicksort is not a stable algorithm, as it will not necessarily preserve the relative order of equal elements in its sorted output. 
3.4 Bucket Sort
	Unlike mergesort, quicksort, and insertion sort, bucket sort is a uniformly distributed-based sorting algorithm that operates by dividing the input into discrete “buckets” and then sorting each bucket individually. To explore it further, we can break down each step and look at it in a more detailed fashion. Initially, the algorithm will take the input data and then create the buckets based on the range of values in the input array. The range is essential for creating an appropriate number of buckets and dividing the input values into distinct intervals. These distinct intervals are used to create a corresponding array of empty buckets, and the number of buckets should be chosen to ensure a balanced distribution of the elements. Next the algorithm will iterate through the input array, and assign each element to the corresponding bucket based on its value. This is typically done by mapping the value of the appropriate bucket index using a simple mathematical calculation. After the bucket index has been calculated for each element, the elements will be placed into their corresponding buckets determined by its value. Note that elements with an equal or very similar value will end up in the same bucket. The next step is to use a stable sorting algorithm, such as insertion sort or merge sort, to sort each individual bucket. There are factors that can affect the choice of which algorithm to use, such as the size of each bucket and the characteristics of the data within each bucket. For example, if the size of the buckets are small, it would be better to use insertion sort rather than merge sort. The final step of the algorithm is to concatenate the sorted buckets. The order of the concatenation is determined by the order of the buckets, and this step involves appending or merging the sorted buckets to create the overall sorted array. Each individual element in every bucket is inserted back into the original array, and once an element has been copied it is removed from its respective bucket. This process is then repeated for all the buckets until each element has been removed (“Bucket Sort - Data Structure and Algorithm Tutorials”).
	As mentioned above, bucket sort is a well-suited algorithm for uniformly distributed data. By dividing the input into buckets and placing the elements of the array into buckets based on their values, it takes advantage of the even distribution to achieve efficient sorting. Bucket sort has a best case time complexity of O(n), which occurs when the elements follow a uniform distribution in the buckets, with very similar elements in each bucket (Upadhyay). It should be noted that if the elements with the buckets are already sorted, the time complexity will increase. Also, if an insertion sort method is implemented the total computational complexity will also be linear, being represented by O(n + k), where O(n) is the time complexity of creating the buckets, and O(k) is the complexity of sorting the bucket elements using an algorithm with a best case of linear time complexity (Upadhyay). Bucket sort’s average case time complexity is represented by the same as its best case scenario, O(n). This happens when the array’s elements are distributed at random, and regardless of whether the distribution of elements in the input is uniform or not, bucket sort will hold a linear time (Upadhyay). The algorithm’s worst case scenario is represented by O(n2), and this occurs when the elements from the input are similar in value, meaning that they will likely be placed in the same bucket resulting in a larger bucket size, or an uneven distribution of the elements (Upadhyay). 
	The stability of a bucket sort implementation solely depends on whether a stable or an unstable sorting algorithm is used within it. Earlier we provided the example of implementing an insertion sort into a bucket sort algorithm, and since insertion sort is stable, that implementation of bucket sort will also be stable. However, if an implementation of quicksort was used, the result bucket sort algorithm would be considered unstable due to the fact that quicksort itself is unstable. Bucket sort also lacks adaptability, as it is most useful when the input is uniformly distributed. If the input data is not uniformly distributed, the efficiency of the sorting algorithm will most likely decay. This is mostly due to the fact that if the majority of elements are in the same range, the bucket sizes will not be even, and they will be underutilized in the algorithm. However, bucket sort can be adjusted to handle varying input sizes, which can be important and useful. The number of buckets and their sizes can be adjusted based on the input array, which in turn can contribute to the algorithm’s scalability. 
Our Implementations
	Now that we have explored the algorithmic background and the important characteristics of the methods we have chosen to implement into our project, the source code and our actual implementations of the functions can be explored. In this section we will do a deep dive into our functions, and their helper functions, as well as the input scenarios we have chosen and their respective outputs. We will also look at the benchmark function we have utilized to time each sorting algorithm, and be able to compare those outputs with the findings we have made in our research. To start, we will first look at our implementation of insertion sort. 
4.1 Our Insertion Sort
In our implementation of insertion sort seen on the left, we pass in a reference variable named list that is linked to a vector. The vector is filled with integers from the input file chosen by the user through the use of command line arguments. The “&” signifies that our function works directly with the original vector passed to it rather than creating a copy, which makes sense considering insertion sort is a stable function and does not require access to additional memory proportional to the size of the input. We then initialize an outer “for loop”, in which the loop iterates through each element of the list, starting from the index of the second element (index 1). This loop is responsible for considering each unsorted element in the list. Inside the loop we declare a variable named “temp” which is initialized to hold the value of the current unsorted element, which is the right number in comparison. We then initialize the variable j to be the index of the number to the left of temp, and using j we construct a “while loop”, in which the sorting process is carried out. This loop is responsible for comparing the current unsorted element (‘temp’) with the elements to its left. It continues to iterate as long as the left element is greater than ‘temp’ and ‘j’ (the index of the left element) is greater than or equal to 0. After the while loop completes, ‘temp’ is assigned to the correct position in the sorted sequence. This is accomplished by placing it to the right of the last shifted element (at ‘list[j + 1]’). This process is repeated until all the elements in the list are appropriately sorted. 
	Shown below is an example of input and output for our insertion sort function.
For this example we will inspect this input file, “sorted.txt”, where the contents of the file are sorted in ascending order.. Located in the screenshot below is the output file “InsertionListSorted.txt” as well as the terminal where the runtime of the function is shown. This output correctly matches up with 


the expectations from our research. This input file was particularly small, only having 20 values. Theoretically, insertion sort should be the fastest out of the four algorithms because it performs very efficiently with small amounts of input data. As we can see from the picture, the insertion sort function was benchmarked at 441 nanoseconds, followed by quicksort at 1443 nanoseconds, which is considerably slower. 
4.2 Our Mergesort

	
	
	



Seen above is part of our implementation of our mergesort function. In the public mergesort function seen at the top, a reference to a list of numbers is passed in to the function. The function then calls mergeHelper to initiate the sorting process. The mergeHelper function is a recursive helper function that divides the input vector into smaller subproblems until each subproblem contains only one element. We include an if statement where if the index of the left boundary is smaller than the index of the right boundary of the current subproblem, an integer mid is created which will get the middle of the vector. The function then uses recursion to sort the left and right subproblems and then finally calls the merge function to merge the sorted subproblems.
	Below is the merge function, which performs the merging step in the mergesort algorithm. We start by passing in the reference to the list of input values, and the left, right and middle indices. We then declare two variables, ‘x1’ and ‘x2’, which represent the size of the left and right subproblems respectively. We then create two temporary vectors for the left and right subproblems, and pass in the values for the left and right subproblems into the temporary vectors by using two for loops. We then initialize variables ‘i’ and ‘j’ to zero to represent the left and right vector indices and use them to traverse through different portions of the data. They are then used in the while loops to compare and merge elements from the left and right vectors back into the original vector (‘list’). These loops continue until either the left or right vector has been exhausted. Below is an example of a different one of our input files, “reverse.txt”, which contains 10000 lines of integers sorted in reversed order. The output file contains the correct sorted order for all 10000 lines of integer values, and the function is benchmarked at 4550844 nanoseconds, making it much more reliable than quicksort, clocked at 587473177 nanoseconds, which matches up with our research claiming that quicksort is not optimal when dealing with a reversed sorted input. Mergesort performs in a timely manner here, also matching up with our research. 



4.3 Our Quicksort
In the above picture, our implementation of quicksort is shown. The public method is shown at the top, and it takes a reference to the input vector as a parameter. It then calls the quickHelper function with the base parameters for initialization. The quickHelper function is a recursive helper method for our quicksort implementation that handles the partitioning and sorting of subarrays. We declare the recursive base case in the ‘if statement’ of the function where we say that is the low index is less than the high index, we will declare a variable ‘pivotIdx’ equal to the function call of partition with parameters: list, low, and high, representing the list vector, the low index, and the high index. Followed by that function call are recursive function calls to quickHelper for the subarrays before and after the pivot. The partition function takes in a reference to the list vector, and the low and high indices as parameters. We declare a variable ‘pivot’ and set it equal to the high index of the list, one of the more viable approaches to quicksorting. We then set the variable ‘i’ equal to the low index of the smaller element. Then using a ‘for loop’ we iterate through the elements from a variable ‘j’ equaling the low index, to the high index - 1. Then by using an ‘if statement’ we check if the current element is smaller than the pivot, and if it is, we increment the index of the smaller element and use std::swap to swap the pivot element with the element at the next index of the smaller element. If the current element is not smaller than the pivot, we swap the pivot element with the element at the next index of the smaller element, and return the index of the pivot element. 
Shown to the left is the input file “partial.txt” which contains a partially sorted list of integers. It is not completely clear in the screenshot, but the first 500 lines of this input file are unsorted integers, and the other 500 values are integers sorted from 1 to 500 in increasing order. Shown below is the output file “QuickSortedList.txt”. As shown, the integers are correctly sorted in ascending order, and the benchmarked time of the function is the slowest by a large margin, as it clocked in around four million nanoseconds.This result is as expected, as we found that in our research quicksort is not an optimal algorithm for data that has already been sorted. Our insertion sort algorithm performs extremely well with this input, which was to be expected. 
4.4 Our Bucket Sort
Shown in the image to the left is part of our implementation of bucket sort. We start by writing a function to find the minimum value in the given vector. It takes the parameter of a reference to the list vector, and first uses an ‘if statement’ to determine if the vector it has been given is empty. If the vector is not empty, we declare an unsigned integer variable ‘min’ and set it equal to the first element of the vector. Then, using a for loop, it iterates through the vector from another unsigned integer variable, ‘num’, which is set equal to zero, until the list size is greater than num. While iterating through the vector, it compares each element with ‘num’, and using the while loop, if ‘num’ is less than ‘min’ it updates ‘min’ to the value of ‘num’. After the loop concludes, it returns the minimum value found. The next function ‘findMax’ follows the exact same logic except for the if statement, where if ‘num’ is greater than ‘max’, it will update ‘max’ with the current value of ‘num’ and then return the maximum value. 

Located on the left is our implementation of the bucket sort function. It takes in the parameter of a reference to the input vector ‘list’, and starts by calling the helper functions ‘findMin’ and ‘findMax’. We then declare an integer variable ‘size’ and set it equal to the maximum value, subtracted by the minimum value, and then dividing that value by the list’s size - 1. This calculates the size of each bucket based on the range of input value in the vector. We then create the buckets by initializing a two dimensional vector to store the elements in different buckets. The number of buckets is set equal to the size of the input vector. We then use a for loop starting at the unsigned integer ‘val’, which equals one, until the list size is greater than ‘val’, to iterate through the input vector, and the calculate the index of the corresponding bucket for each element, and then using the ‘push_back’ command we push the element into that bucket. In the next for loop we initialize a reference named ‘bucket’ that will be used to refer to each element in the range. We use the ‘auto’ keyword to automatically deduce the type of each element in range. In this loop we call the insertion sort function to individually sort the elements within the specified bucket. The final portion of our bucket sort function is a for loop to concatenate the sorted buckets back into the original vector. We initialize the variable ‘temp’ to zero, and have it serve as an index to place elements back into the original vector. 
	Shown below on the left  is the input file “random.txt”, the last of our four input files. This file contains twenty thousand lines of input, all being random integers in no particular order. Also shown on the right is the output file “BucketSortedList.txt”.
 


As shown in the above picture, bucket sort is benchmarked here at roughly 5.5 million nanoseconds, which makes sense due to the fact that this input file is random and bucket sort's best case time complexity is O(n), making bucket sort a favorable choice for this input. It is faster in this scenario than mergesort and quicksort.
4.5 Benchmarking Function
Seen in the snippet above is a simple benchmarking function we implemented to time the run time of each algorithm. Our function takes in a parameter ‘void*(sortFunction)(std::vector<int>&)’ which is a pointer to a function that takes a reference to a vector and returns ‘void’. This is a placeholder for our sorting algorithms. It also takes in a reference to the list of input. We start the time by using ‘auto start = std::chrono::high_resolution_clock::now();’ and it records the current time as the starting point for measuring the execution time of the algorithm. In the next line, we call the sorting algorithm by using the function pointer ‘sortFunction’ and invoke it with the provided input list. In the following line, we stop the clock by setting a variable of type auto, ‘stop’, as the current stoppage time. In the next line, we calculate the duration of the algorithm’s execution by subtracting the start and stop times, and we lastly print the benchmarked times of each function to the terminal.





 Table of Contributions



Code
Slides
Paper
Austin
Functions: runTime, quickSort, quickHelper, partition
Header File
Slides 9,10
Sections 1,3,4,5 
Josh
Functions: merge, printList, mergeHelper, mergeSort
Header File, creation of input files, worked on main, readMe file
Slides 6-8, 15
Sections 1,2&6
Matt
Functions: saveSortedToFile, insertionSort,
Header File, in main: printing output to files
Slides 2-5
Sections 1,2&6
Ruben
Functions: findMin, findMax, bucketSort
Header File
Slides 11-14
Sections 1,2&6


Conclusion
	In conclusion, having a strong understanding of sorting algorithms provides a fundamental understanding of their role in computer science and real world applications. Sorting algorithms are essential programming tools that transform datasets into organized and usable information. As demonstrated by the implemented algorithms; insertion sort, mergesort, quicksort, and bucket sort, the selection of a sorting method depends on the unique characteristics of the data and the specific application. Each algorithm comes with its strengths and limitations, such as time and space complexities, stability, and adaptability/suitability. Sorting algorithms are a fundamental building block when it comes to working with data sets and having a deep understanding is crucial for all computer science applications along with many other professions.



















Works Cited
“Bucket Sort - Data Structures and Algorithms Tutorials.” GeeksforGeeks, GeeksforGeeks, 8 
	Sept. 2023, www.geeksforgeeks.org/bucket-sort-2/. 
Eldridge, Stephen. “Sorting Algorithm.” Encyclopædia Britannica, Encyclopædia Britannica, 
	inc., 2 Nov. 2023, www.britannica.com/technology/sorting-algorithm. 
Eldridge, Stephen. “Time Complexity.” Encyclopædia Britannica, Encyclopædia Britannica, 
	inc., www.britannica.com/science/time-complexity. Accessed 1 Dec. 2023. 
“External Sorting.” GeeksforGeeks, GeeksforGeeks, 10 Jan. 2023, 
	www.geeksforgeeks.org/external-sorting/. 
“Insertion Sort - Data Structure and Algorithm Tutorials.” GeeksforGeeks, 
	GeeksforGeeks, 31 May 2023, www.geeksforgeeks.org/insertion-sort/. 
“How Results Are Automatically Generated.” Google, Google, 
	www.google.com/search/howsearchworks/how-search-works/ranking-results/. Accessed 
	3 Dec. 2023. 
“Insertion Sort Algorithm.” Www.Javatpoint.Com, JavaTpoint, 2011, 
	www.javatpoint.com/insertion-sort. 
“Merge Sort - Data Structure and Algorithms Tutorials.” GeeksforGeeks, GeeksforGeeks, 28 
	Nov. 2023, www.geeksforgeeks.org/merge-sort/. 
Mohsin, Maryam. “10 Google Search Statistics You Need to Know in 2023.” Oberlo, Oberlo, 25 
	Apr. 2023, www.oberlo.com/blog/google-search-statistics. 
“Quicksort - Data Structure and Algorithm Tutorials.” GeeksforGeeks, GeeksforGeeks, 16 Oct. 
	2023, www.geeksforgeeks.org/quick-sort/. 
“Server-Side and Client-Side Sorting and Filtering.” Server-Side and Client-Side Sorting and 
	Filtering, Devart,
docs.devart.com/data-compare-for-oracle/working-with-data-in-data-editor/server-side.html. 
	Accessed 2 Dec. 2023. 
Siddhi. “What Is Information Retrieval?” GeeksforGeeks, GeeksforGeeks, 19 Sept. 2023, 
	www.geeksforgeeks.org/what-is-information-retrieval/. 
Stringer, Thomas. “Sorting Algorithm Stability - What It Is and When It Matters.” TStringer, 13 
	Dec. 2020, trstringer.com/sorting-stability/. 
“What Are In-Place and Out-of-Place Algorithms?” Educative, Educative Inc, 
	www.educative.io/answers/what-are-in-place-and-out-of-place-algorithms. Accessed 2 
	Dec. 2023. 
Upadhyay, Soni. “Bucket Sort Algorithm: Time Complexity & Pseudocode: Simplilearn.” 
	Simplilearn.Com, Simplilearn, 23 Feb. 2023, 
	www.simplilearn.com/tutorials/data-structure-tutorial/bucket-sort-algorithm. 

